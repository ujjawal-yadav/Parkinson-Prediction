# -*- coding: utf-8 -*-
"""parkinson.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tbf38OYpsWsueTULaoTpJ2My12lVhvc4
"""

import numpy as np
import cv2
from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Dropout, Flatten, Dense, MaxPool2D
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.initializers import glorot_uniform
from tensorflow.keras.optimizers import Adam, SGD
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from tensorflow.keras.regularizers import l2
import tensorflow as tf
import pandas as pd
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint



import os
import numpy as np
from PIL import Image
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Define the path to the dataset
dataset_path = '/content/drive/My Drive/pred'

# Function to load and resize images from a folder into a numpy array
def load_and_resize_images_from_folder(folder_path, target_shape, label):
    images = []
    labels = []
    for filename in os.listdir(folder_path):
        img_path = os.path.join(folder_path, filename)
        if os.path.isfile(img_path):
            try:
                img = Image.open(img_path)
                img = img.convert('RGB')
                # Resize image to target shape
                img = img.resize(target_shape)
                # Convert image to numpy array
                img_array = np.array(img)
                images.append(img_array)
                labels.append(label)
            except Exception as e:
                print(f"Error loading image: {img_path}, {e}")
    return np.array(images), np.array(labels)

# Define the folder structure
folders = ['spiral', 'wave']
classes = ['healthy', 'parkinson']
train_subfolder = 'training'
target_shape = (256, 256)  # Adjust the target shape as needed

# Load and resize spiral image training data
spiral_training_healthy, spiral_training_healthy_labels = load_and_resize_images_from_folder(os.path.join(dataset_path, 'spiral', train_subfolder, 'healthy'), target_shape, 0)
spiral_training_parkinson, spiral_training_parkinson_labels = load_and_resize_images_from_folder(os.path.join(dataset_path, 'spiral', train_subfolder, 'parkinson'), target_shape, 1)

# Load and resize wave image training data
wave_training_healthy, wave_training_healthy_labels = load_and_resize_images_from_folder(os.path.join(dataset_path, 'wave', train_subfolder, 'healthy'), target_shape, 0)
wave_training_parkinson, wave_training_parkinson_labels = load_and_resize_images_from_folder(os.path.join(dataset_path, 'wave', train_subfolder, 'parkinson'), target_shape, 1)

# Example usage:
# Now you have eight numpy arrays:
# - spiral_training_healthy
# - spiral_training_parkinson
# - wave_training_healthy
# - wave_training_parkinson
# And their corresponding label arrays:
# - spiral_training_healthy_labels
# - spiral_training_parkinson_labels
# - wave_training_healthy_labels
# - wave_training_parkinson_labels

import numpy as np

# Concatenate spiral data and labels
spiral_training_data = np.concatenate((spiral_training_healthy, spiral_training_parkinson), axis=0)
spiral_training_labels = np.concatenate((spiral_training_healthy_labels, spiral_training_parkinson_labels), axis=0)

# Concatenate wave data and labels
wave_training_data = np.concatenate((wave_training_healthy, wave_training_parkinson), axis=0)
wave_training_labels = np.concatenate((wave_training_healthy_labels, wave_training_parkinson_labels), axis=0)

# Example usage:
# Now you have two concatenated numpy arrays for spiral and wave data respectively:
# - spiral_training_data
# - wave_training_data
# And their corresponding concatenated label arrays:
# - spiral_training_labels
# - wave_training_labels


del spiral_training_healthy
del spiral_training_parkinson
del wave_training_healthy
del  wave_training_parkinson
# And their corresponding label arrays:
del spiral_training_healthy_labels
del spiral_training_parkinson_labels
del wave_training_healthy_labels
del wave_training_parkinson_labels



print (spiral_training_data.shape)
print (wave_training_data.shape)

import matplotlib.pyplot as plt

# Function to plot random images with corresponding labels
def plot_random_images_with_labels(images, labels, num_images=10):
    fig, axes = plt.subplots(2, num_images // 2, figsize=(15, 6))
    fig.subplots_adjust(hspace=0.3, wspace=0.3)
    for i, ax in enumerate(axes.flat):
        idx = np.random.randint(len(images))
        ax.imshow(images[idx])
        ax.set_title(f"Label: {labels[idx]}")
        ax.axis('off')
    plt.show()

# Plot random images and labels from spiral data
plot_random_images_with_labels(spiral_training_data, spiral_training_labels)

# Plot random images and labels from wave data
plot_random_images_with_labels(wave_training_data, wave_training_labels)

import matplotlib.pyplot as plt
import seaborn as sns

# Get unique labels and their counts for spiral data
unique_spiral_train, spiral_count = np.unique(spiral_training_labels, return_counts=True)

# Plot for spiral data
plt.figure(figsize=(10, 6))
sns.barplot(x=unique_spiral_train, y=spiral_count)
plt.title("Number of training images per category for Spiral data")
plt.xlabel("Category")
plt.ylabel("Count")
plt.xticks([0, 1], ['Healthy', 'Parkinson'])
plt.show()

# Get unique labels and their counts for wave data
unique_wave_train, wave_count = np.unique(wave_training_labels, return_counts=True)

# Plot for wave data
plt.figure(figsize=(10, 6))
sns.barplot(x=unique_wave_train, y=wave_count)
plt.title("Number of training images per category for Wave data")
plt.xlabel("Category")
plt.ylabel("Count")
plt.xticks([0, 1], ['Healthy', 'Parkinson'])
plt.show()

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define data augmentation parameters
data_generator = ImageDataGenerator(
    horizontal_flip=True,
    vertical_flip=True,
    width_shift_range=0.1,
    height_shift_range=0.1,
    brightness_range=[0.3, 1.15],
    shear_range=0.2,
    zoom_range=0.2,
    fill_mode='nearest',
    preprocessing_function=None,
    validation_split=0.0,
    dtype='float32'
)

# Convert x_train and y_train to lists
x_train_list = list(spiral_training_data)
y_train_list = list(spiral_training_labels)

# Augment the data
x_aug_train = []
y_aug_train = []

for i, img in enumerate(x_train_list):
    img = np.expand_dims(img, axis=0)
    aug_iter = data_generator.flow(img, batch_size=1, shuffle=True)
    for _ in range(65):  # Augment each image 70 times
        aug_image = next(aug_iter)[0].astype('uint8')
        x_aug_train.append(aug_image)
        y_aug_train.append(y_train_list[i])  # Keep the label same

# Concatenate augmented data with original data
spiral_training_data_augmented = np.concatenate((spiral_training_data, np.array(x_aug_train)), axis=0)
spiral_training_labels_augmented = np.concatenate((spiral_training_labels, np.array(y_aug_train)), axis=0)

# Now, spiral_training_data_augmented and spiral_training_labels_augmented contain the augmented data

# Example usage:
# Now you can use spiral_training_data_augmented and spiral_training_labels_augmented in your training process

print( spiral_training_data_augmented.shape)

import matplotlib.pyplot as plt
import numpy as np

# Function to plot random images with corresponding labels
def plot_random_images_with_labels(images, labels, label_names, num_images=10):
    fig, axes = plt.subplots(2, num_images // 2, figsize=(15, 6))
    fig.subplots_adjust(hspace=0.3, wspace=0.3)
    for i, ax in enumerate(axes.flat):
        idx = np.random.randint(len(images))
        ax.imshow(images[idx])
        ax.set_title(f"Label: {label_names[labels[idx]]}")
        ax.axis('off')
    plt.show()

# Define label names
label_names = {0: 'Healthy', 1: 'Parkinson'}

# Plot random images and labels
plot_random_images_with_labels(spiral_training_data_augmented, spiral_training_labels_augmented, label_names)

# Define data augmentation parameters for wave data
wave_data_generator = ImageDataGenerator(
    horizontal_flip=True,
    vertical_flip=True,
    width_shift_range=0.1,
    height_shift_range=0.1,
    brightness_range=[0.5, 1.15],
    shear_range=0.2,
    zoom_range=0.2,
    fill_mode='nearest',
    preprocessing_function=None,
    validation_split=0.0,
    dtype='float32'
)

# Convert wave training data and labels to lists
wave_training_data_list = list(wave_training_data)
wave_training_labels_list = list(wave_training_labels)

# Augment the wave data
wave_augmented_data = []
wave_augmented_labels = []

# Define the number of augmentations per image
num_augmentations = 4752 // len(wave_training_data_list)

for i, img in enumerate(wave_training_data_list):
    img = np.expand_dims(img, axis=0)
    aug_iter = wave_data_generator.flow(img, batch_size=1, shuffle=True)
    for _ in range(num_augmentations):
        aug_image = next(aug_iter)[0].astype('uint8')
        wave_augmented_data.append(aug_image)
        wave_augmented_labels.append(wave_training_labels_list[i])  # Keep the label same

# Convert augmented wave data and labels to arrays
wave_augmented_data = np.array(wave_augmented_data)
wave_augmented_labels = np.array(wave_augmented_labels)

# Now, wave_augmented_data and wave_augmented_labels contain the augmented wave data and labels

print(wave_augmented_data.shape)

# Function to plot random images from both spiral and wave data with corresponding labels
def plot_random_images_with_labels(images1, labels1, images2, labels2, label_names, num_images=12):
    fig, axes = plt.subplots(2, num_images // 2, figsize=(15, 8))
    fig.subplots_adjust(hspace=0.3, wspace=0.3)
    for i, ax in enumerate(axes.flat):
        if i < num_images // 2:  # Plot from spiral data
            idx = np.random.randint(len(images1))
            ax.imshow(images1[idx])
            ax.set_title(f"Spiral - Label: {label_names[labels1[idx]]}")
        else:  # Plot from wave data
            idx = np.random.randint(len(images2))
            ax.imshow(images2[idx])
            ax.set_title(f"Wave - Label: {label_names[labels2[idx]]}")
        ax.axis('off')
    plt.show()

# Define label names
label_names = {0: 'Healthy', 1: 'Parkinson'}

# Plot random images and labels from both spiral and wave data
plot_random_images_with_labels(spiral_training_data_augmented, spiral_training_labels_augmented,
                               wave_augmented_data, wave_augmented_labels,
                               label_names)

import cv2
from sklearn.preprocessing import LabelEncoder

# Preprocess augmented spiral data
x_aug_spiral = []
for i in range(len(spiral_training_data_augmented)):
    img = spiral_training_data_augmented[i]
    img = cv2.resize(img, (128, 128))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    x_aug_spiral.append(img)

x_aug_spiral = np.array(x_aug_spiral) / 255.0  # Normalize

# Preprocess augmented wave data
x_aug_wave = []
for i in range(len(wave_augmented_data)):
    img = wave_augmented_data[i]
    img = cv2.resize(img, (128, 128))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    x_aug_wave.append(img)

x_aug_wave = np.array(x_aug_wave) / 255.0  # Normalize

# Convert labels using LabelEncoder
label_encoder = LabelEncoder()
y_aug_spiral = label_encoder.fit_transform(spiral_training_labels_augmented)

label_encoder = LabelEncoder()
y_aug_wave = label_encoder.fit_transform(wave_augmented_labels)

# Now, x_aug_spiral, y_aug_spiral, x_aug_wave, and y_aug_wave contain the preprocessed augmented spiral and wave data

import matplotlib.pyplot as plt
import numpy as np

# Function to plot random images from both spiral and wave data with corresponding labels
def plot_random_images_with_labels(images1, labels1, images2, labels2, label_names, num_images=12):
    fig, axes = plt.subplots(2, num_images // 2, figsize=(15, 8))
    fig.subplots_adjust(hspace=0.3, wspace=0.3)
    for i, ax in enumerate(axes.flat):
        if i < num_images // 2:  # Plot from spiral data
            idx = np.random.randint(len(images1))
            ax.imshow(images1[idx], cmap='gray')
            ax.set_title(f"Spiral - Label: {label_names[labels1[idx]]}")
        else:  # Plot from wave data
            idx = np.random.randint(len(images2))
            ax.imshow(images2[idx], cmap='gray')
            ax.set_title(f"Wave - Label: {label_names[labels2[idx]]}")
        ax.axis('off')
    plt.show()

# Define label names
label_names = {0: 'Healthy', 1: 'Parkinson'}

# Plot random images and labels from both spiral and wave data
plot_random_images_with_labels(x_aug_spiral, y_aug_spiral,
                               x_aug_wave, y_aug_wave,
                               label_names)

del spiral_training_data_augmented
del spiral_training_labels_augmented
del wave_augmented_data
del wave_augmented_labels

# Load and resize spiral image testing data
spiral_testing_healthy, spiral_testing_healthy_labels = load_and_resize_images_from_folder(os.path.join(dataset_path, 'spiral', 'testing', 'healthy'), target_shape, 0)
spiral_testing_parkinson, spiral_testing_parkinson_labels = load_and_resize_images_from_folder(os.path.join(dataset_path, 'spiral', 'testing', 'parkinson'), target_shape, 1)

# Load and resize wave image testing data
wave_testing_healthy, wave_testing_healthy_labels = load_and_resize_images_from_folder(os.path.join(dataset_path, 'wave', 'testing', 'healthy'), target_shape, 0)
wave_testing_parkinson, wave_testing_parkinson_labels = load_and_resize_images_from_folder(os.path.join(dataset_path, 'wave', 'testing', 'parkinson'), target_shape, 1)

print (wave_testing_healthy.shape)

# Define data augmentation parameters for wave data
wave_data_generator = ImageDataGenerator(
    horizontal_flip=True,
    vertical_flip=True,
    width_shift_range=0.1,
    height_shift_range=0.1,
    brightness_range=[0.5, 1.15],
    shear_range=0.2,
    zoom_range=0.2,
    fill_mode='nearest',
    preprocessing_function=None,
    validation_split=0.0,
    dtype='float32'
)

# Define data augmentation parameters for spiral data
spiral_data_generator = ImageDataGenerator(
    horizontal_flip=True,
    vertical_flip=True,
    width_shift_range=0.1,
    height_shift_range=0.1,
    brightness_range=[0.3, 1.15],
    shear_range=0.2,
    zoom_range=0.2,
    fill_mode='nearest',
    preprocessing_function=None,
    validation_split=0.0,
    dtype='float32'
)

# Function to augment images and labels
def augment_data(generator, images, labels, num_images):
    augmented_images = []
    augmented_labels = []
    for i in range(len(images)):
        img = np.expand_dims(images[i], axis=0)
        label = labels[i]
        aug_iter = generator.flow(img, batch_size=1, shuffle=True)
        for _ in range(num_images):
            aug_image = next(aug_iter)[0].astype('uint8')
            augmented_images.append(aug_image)
            augmented_labels.append(label)
    return np.array(augmented_images), np.array(augmented_labels)

# Augment wave testing data
wave_testing_data_augmented, wave_testing_labels_augmented = augment_data(
    wave_data_generator,
    np.concatenate((wave_testing_healthy, wave_testing_parkinson)),
    np.concatenate((wave_testing_healthy_labels, wave_testing_parkinson_labels)),
    num_images=289
)

# Augment spiral testing data
spiral_testing_data_augmented, spiral_testing_labels_augmented = augment_data(
    spiral_data_generator,
    np.concatenate((spiral_testing_healthy, spiral_testing_parkinson)),
    np.concatenate((spiral_testing_healthy_labels, spiral_testing_parkinson_labels)),
    num_images=289
)

import cv2
from sklearn.preprocessing import LabelEncoder

# Preprocess augmented spiral testing data
x_aug_spiral_preprocessed = []
for i in range(len(spiral_testing_data_augmented)):
    img = spiral_testing_data_augmented[i]
    img = cv2.resize(img, (128, 128))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    x_aug_spiral_preprocessed.append(img)

x_aug_spiral_preprocessed = np.array(x_aug_spiral_preprocessed) / 255.0  # Normalize

# Preprocess augmented wave testing data
x_aug_wave_preprocessed = []
for i in range(len(wave_testing_data_augmented)):
    img = wave_testing_data_augmented[i]
    img = cv2.resize(img, (128, 128))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    x_aug_wave_preprocessed.append(img)

x_aug_wave_preprocessed = np.array(x_aug_wave_preprocessed) / 255.0  # Normalize

# Convert labels using LabelEncoder for augmented spiral and wave testing data
label_encoder = LabelEncoder()
y_aug_spiral_preprocessed = label_encoder.fit_transform(spiral_testing_labels_augmented)

label_encoder = LabelEncoder()
y_aug_wave_preprocessed = label_encoder.fit_transform(wave_testing_labels_augmented)

# Now, x_aug_spiral_preprocessed, y_aug_spiral_preprocessed, x_aug_wave_preprocessed, and y_aug_wave_preprocessed
# contain the preprocessed augmented spiral and wave testing data

del spiral_testing_data_augmented
del spiral_testing_labels_augmented
del wave_testing_data_augmented
del wave_testing_labels_augmented

import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Dropout, Flatten, Dense, MaxPool2D
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.utils import to_categorical

def parkinson_disease_detection_model(input_shape=(128, 128, 1)):
    regularizer = tf.keras.regularizers.l2(0.001)
    model = Sequential()
    model.add(Input(shape=input_shape))
    model.add(Conv2D(128, (5, 5), padding='same', strides=(1, 1), name='conv1', activation='relu',
                     kernel_initializer='glorot_uniform', kernel_regularizer=regularizer))
    model.add(MaxPool2D((9, 9), strides=(3, 3)))

    model.add(Conv2D(64, (5, 5), padding='same', strides=(1, 1), name='conv2', activation='relu',
                     kernel_initializer='glorot_uniform', kernel_regularizer=regularizer))
    model.add(MaxPool2D((7, 7), strides=(3, 3)))

    model.add(Conv2D(32, (3, 3), padding='same', strides=(1, 1), name='conv3', activation='relu',
                     kernel_initializer='glorot_uniform', kernel_regularizer=regularizer))
    model.add(MaxPool2D((5, 5), strides=(2, 2)))

    model.add(Conv2D(32, (3, 3), padding='same', strides=(1, 1), name='conv4', activation='relu',
                     kernel_initializer='glorot_uniform', kernel_regularizer=regularizer))
    model.add(MaxPool2D((3, 3), strides=(2, 2)))

    model.add(Flatten())
    model.add(Dropout(0.5))
    model.add(Dense(64, activation='relu', kernel_initializer='glorot_uniform', name='fc1'))
    model.add(Dropout(0.5))
    model.add(Dense(2, activation='softmax', kernel_initializer='glorot_uniform', name='fc3'))

    optimizer = Adam(3.15e-5)
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Define a callback to save the model weights with the best validation accuracy
checkpoint = ModelCheckpoint(filepath='best_model_weights.h5',
                             monitor='val_accuracy',
                             verbose=1,
                             save_best_only=True,
                             mode='max')


# Reshape the augmented spiral training data
x_aug_spiral = x_aug_spiral.reshape((-1, 128, 128, 1))

# Reshape the preprocessed augmented spiral testing data
x_aug_spiral_preprocessed = x_aug_spiral_preprocessed.reshape((-1, 128, 128, 1))

# Convert labels to one-hot encoding
y_aug_spiral_onehot = to_categorical(y_aug_spiral)

# Convert preprocessed testing labels to one-hot encoding
y_aug_spiral_preprocessed_onehot = to_categorical(y_aug_spiral_preprocessed)

# Create the model
model = parkinson_disease_detection_model()

# Train the model using augmented spiral training data and preprocessed testing data for validation
hist = model.fit(x_aug_spiral, y_aug_spiral_onehot, batch_size=128, epochs=150, validation_data=(x_aug_spiral_preprocessed, y_aug_spiral_preprocessed_onehot),callbacks=[checkpoint])

best_model = parkinson_disease_detection_model()
best_model.load_weights('best_model_weights.h5')

loss, accuracy = best_model.evaluate(x_aug_spiral_preprocessed, y_aug_spiral_preprocessed_onehot)

